# Linear Regression
With Linear Regression, we aim to fit a line (or hyperplane) to a set of input/output data points.Regression can be used in machine learning to drive home a particular point about the data under study, visually study a correlation, learn about a model to make percise predictions, and more. Least squares and Least Absolute Deviations are among these techniques.

### Second order Optimization
Second-order optimization algorithms deal with the first and second derivatives or the gradient and Hessian of functions. These define the curvature and second order Tylor expansions.
To know if a single value function g(w) is concave or convex at a point v, we check its curvature or second derivative information at that point. If the second derivative is >=0 or <= 0 then its convex or concave at point v. The function g(w) is convex everywhere if the second derivative is always positive. The matrix g(w) is convex everywhere if the second derivative is positive 

